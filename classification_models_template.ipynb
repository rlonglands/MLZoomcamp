{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models - Template Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code examples for logistic regression and tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace(' ', '_').str.lower()\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find correlations in the data - optional\n",
    "matrix = df.corr()\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split df and set random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=2)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=2)\n",
    "len(df_full_train), len(df_test), len(df_train), len(df_val)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "target = #fill target column\n",
    "y_full_train = df_full_train[target]\n",
    "y_test = df_test[target]\n",
    "y_train = df_train[target]\n",
    "y_val = df_val[target]\n",
    "\n",
    "del df_full_train[target]\n",
    "del df_test[target]\n",
    "del df_train[target]\n",
    "del df_val[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Logistic Regression and validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "train_dict = df_train.to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "val_dict = df_val.to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', C=1, max_iter=1000, random_state=2)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(X_val)[:,1]\n",
    "y_pred_bool = (y_pred >= 0.5).astype(int)\n",
    "(y_val == y_pred_bool).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame()\n",
    "df_pred['actual'] = y_val\n",
    "df_pred['probability'] = y_pred\n",
    "df_pred['prediction'] = y_pred_bool.astype(int)\n",
    "df_pred['correct'] = df_pred.prediction == df_pred.actual\n",
    "df_pred.correct.mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation - AUC, thresholds, f-score, kfold to refine c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "thresholds = np.arange(0, 1.0, 0.01)\n",
    "prec = []\n",
    "rec = []\n",
    "for t in thresholds:\n",
    "\n",
    "    actual_positive = (y_val == 1)\n",
    "    actual_negative = (y_val == 0)\n",
    "\n",
    "    predict_positive = (y_pred >= t)\n",
    "    predict_negative = (y_pred < t)\n",
    "\n",
    "    tp = (predict_positive & actual_positive).sum()\n",
    "    tn = (predict_negative & actual_negative).sum()\n",
    "\n",
    "    fp = (predict_positive & actual_negative).sum()\n",
    "    fn = (predict_negative & actual_positive).sum()\n",
    "\n",
    "    p = tp / (tp + fp)\n",
    "    r = tp / (tp + fn)\n",
    "\n",
    "    prec.append(p)\n",
    "    rec.append(r)\n",
    "\n",
    "columns = [ 'thresholds','prec', 'rec']\n",
    "scores = [thresholds, prec, rec]\n",
    "#df['column_name']=pd.Series(arr)\n",
    "df_scores = pd.DataFrame(scores, columns)\n",
    "df_scores = df_scores.T\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_scores.thresholds, df_scores['prec'], label='prec')\n",
    "plt.plot(df_scores.thresholds, df_scores['rec'], label='rec')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(p,r):\n",
    "    a = p*r\n",
    "    b = p+r\n",
    "    c = a/b\n",
    "    f1 = 2*c\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores['f1_score'] = df_scores.apply(lambda x: f1_score(x['prec'],x['rec']), axis = 1)\n",
    "df_scores[df_scores['f1_score'] == df_scores['f1_score'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df_train, y_train, C=1.0):\n",
    "\n",
    "    dicts = df_train.to_dict(orient='records')\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "    \n",
    "    model = LogisticRegression(solver = 'liblinear', C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, dv, model):\n",
    "    dicts = df.to_dict(orient='records')\n",
    "    X = dv.transform(dicts)\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "\n",
    "for C in tqdm([0.01, 0.1, 0.5, 10]):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in kfold.split(df_full_train):\n",
    "        df_train = df_full_train.iloc[train_idx]\n",
    "        df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "        y_train = df_train.above_average.values\n",
    "        y_val = df_val.above_average.values\n",
    "\n",
    "        dv, model = train(df_train, y_train, C=C)\n",
    "        y_pred = predict(df_val, dv, model)\n",
    "\n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(auc)\n",
    "\n",
    "    print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree-based Models - RF and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import export_text\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "train_dicts = df_train.fillna(0).to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "val_dicts = df_val.fillna(0).to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth = 10)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_val)\n",
    "roc_auc_score(y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test different parameters - RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for n in range (10, 201, 10):\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=1, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_val)  \n",
    "    scores.append((n, roc_auc_score(y_val, y_pred)))\n",
    "\n",
    "df_scores = pd.DataFrame(scores, columns=['n_estimators', 'roc_auc_score'])\n",
    "plt.plot(df_scores.n_estimators, df_scores.roc_auc_score)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [10, 15, 20, 25]\n",
    "scores = []\n",
    "for v in values:\n",
    "    for n in range (10, 201, 10):\n",
    "        rf = RandomForestClassifier(n_estimators=n, random_state=1, n_jobs=-1, max_depth = v)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_val)  \n",
    "        scores.append((v, n, roc_auc_score(y_val, y_pred)))\n",
    "        \n",
    "df_depth_scores = pd.DataFrame(scores, columns = ['depth', 'estimators', 'roc_auc_score'] )        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dv.get_feature_names_out()\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist = [(dtrain, 'train'), (dval, 'val')]\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dval, 'val')]\n",
    "xgb_params = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'nthread': 8,\n",
    "    'seed': 1,\n",
    "    'verbosity': 1,\n",
    "}\n",
    "\n",
    "model = xgb.train(xgb_params, dtrain, num_boost_round=200, evals=watchlist)\n",
    "\n",
    "y_pred = model.predict(dval)\n",
    "roc_auc_score(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
